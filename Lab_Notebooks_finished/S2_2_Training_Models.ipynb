{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S2_2_Training_Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jejefragniere/2022_ML_EES/blob/main/Lab_Notebooks_finished/S2_2_Training_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chapter 4 â€“ Training Models**\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td align=middle>\n",
        "    <a target=\"_blank\" href=\"https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb\"> Open the original notebook <br><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "5Tt5C4PoIRl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Setup\n",
        "Let's begin like in the last notebook: importing a few common modules, ensuring MatplotLib plots figures inline and preparing a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so once again we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20.\n",
        "\n",
        "You don't need to worry about understanding everything that is written in this section."
      ],
      "metadata": {
        "id": "666iBNeL8-7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S_OXSp49IOF2"
      },
      "outputs": [],
      "source": [
        "# Python â‰¥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "# Scikit-Learn â‰¥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To make this notebook's output stable across runs\n",
        "rnd_seed = 42\n",
        "rnd_gen = np.random.default_rng(rnd_seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"classification\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Setup"
      ],
      "metadata": {
        "id": "RtuO7Elb9LuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will be working with the [*Iris Flower Dataset*](https://en.wikipedia.org/wiki/Iris_flower_data_set), in which the length and width of both the sepals and petals of three types of Iris flowes were recorded. For reference, these are pictures of the three flowers: <br>\n",
        "\n",
        "<center> In order: Iris Setosa,  Iris Versicolor, and Iris Virginica </center>\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/360px-Kosaciec_szczecinkowaty_Iris_setosa.jpg' height=300 >\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/640px-Iris_versicolor_3.jpg' height=300></img>\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/590px-Iris_virginica.jpg' height=300></img>\n",
        "\n",
        "Photo Credits:[Kosaciec szczecinkowaty Iris setosa](https://en.wikipedia.org/wiki/File:Kosaciec_szczecinkowaty_Iris_setosa.jpg) by [Radomil Binek](https://commons.wikimedia.org/wiki/User:Radomil) licensed under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/deed.en); [Blue flag flower close-up (Iris versicolor)](https://en.wikipedia.org/wiki/File:Iris_versicolor_3.jpg)by Danielle Langlois licensed under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/deed.en); [image of Iris virginica shrevei](https://en.wikipedia.org/wiki/File:Iris_virginica.jpg) by [Frank Mayfield](https://www.flickr.com/photos/33397993@N05) licensed under [CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/deed.en).\n",
        "<br><br>\n",
        "\n",
        "As you can imagine, this dataset is normally used to train *multiclass*/*multinomial* classification algorithms and not *binary* classification algorithms, since there *are* more than 2 classes. \n",
        "\n",
        "\"*Three classes, even!*\" - an observant TA\n",
        "\n",
        "For this exercise, however, we will implement the binary classification algorithm referred to as the *logistic regression* algorithm (also called logit regression)."
      ],
      "metadata": {
        "id": "wKsvLXdmzqD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the Iris Dataset\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Print out some information about the data\n",
        "print(f'Keys in Iris dictionary: \\n{list(iris.keys())}\\n\\n')\n",
        "print(iris.DESCR)\n",
        "\n"
      ],
      "metadata": {
        "id": "emWru72owjEI",
        "outputId": "78213b31-47f7-4751-df3b-a072fe81901d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in Iris dictionary: \n",
            "['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module']\n",
            "\n",
            "\n",
            ".. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 150 (50 in each of three classes)\n",
            "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            "        - sepal length in cm\n",
            "        - sepal width in cm\n",
            "        - petal length in cm\n",
            "        - petal width in cm\n",
            "        - class:\n",
            "                - Iris-Setosa\n",
            "                - Iris-Versicolour\n",
            "                - Iris-Virginica\n",
            "                \n",
            "    :Summary Statistics:\n",
            "\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "                    Min  Max   Mean    SD   Class Correlation\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
            "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
            "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
            "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: 33.3% for each of 3 classes.\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
            "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
            "Machine Learning Repository, which has two wrong data points.\n",
            "\n",
            "This is perhaps the best known database to be found in the\n",
            "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
            "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
            "data set contains 3 classes of 50 instances each, where each class refers to a\n",
            "type of iris plant.  One class is linearly separable from the other 2; the\n",
            "latter are NOT linearly separable from each other.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
            "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
            "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
            "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
            "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
            "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
            "     Structure and Classification Rule for Recognition in Partially Exposed\n",
            "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
            "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
            "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
            "     on Information Theory, May 1972, 431-433.\n",
            "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
            "     conceptual clustering system finds 3 classes in the data.\n",
            "   - Many, many more ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a dictionary with all the data that we need. Let's go ahead and extract the petal length and width to use as input data, storing it in $x$. Then we'll store the labels (i.e., the _targets_) in $y$.\n",
        "### **Q1) Extract the petal length and petal width to use as the input vector $x$, and store the label (i.e., the target data) in $y$**\n",
        "\n",
        "*Hint 1: The \"data\" key is of the dictionary is used to access the features in the dataset.*\n",
        "\n",
        "*Hint 2: The 'target' key is used to access the label information in the dataset.*"
      ],
      "metadata": {
        "id": "kGI9Ruq6BRPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# And load the petal lengths and widths as our input data\n",
        "X = iris[___][:, (__, __)]  # indices for petal length, petal width\n",
        "y = iris[___]"
      ],
      "metadata": {
        "id": "zjwWEZhgBQuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = iris['data'][:,(2,3)]\n",
        "y = iris['target']"
      ],
      "metadata": {
        "id": "KiAY24BpiC-K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset originally has three classes - today we'll only be performing _binary classification_. We need to filter out one of the classes - we'll filter out the _iris setosa_.\n",
        "\n",
        "### **Q2) Filter out the Iris Setosa data from the input and label data to produce a binary classification dataset.**\n",
        "\n",
        "*Hint 1: The target label for iris setosa is 0, the one for iris versicolor is 1, and the one for virginica is 2.*\n",
        "\n",
        "*Hint 2: Numpy's [logical_or](https://numpy.org/doc/stable/reference/generated/numpy.logical_or.html) function will let you combine two sets of conditions elementwise.*\n",
        "\n",
        "*Hint 3: Once you have a boolean array representing the indices where the data corresponds to versicolor and virginica, you can use it as an index for a numpy array to filter out the setosa samples*"
      ],
      "metadata": {
        "id": "S63cMjGKLpd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the indices corresponding to versicolor and virginiza\n",
        "bin_indices = np.______(y==__,y==__)\n",
        "\n",
        "# Select the inputs for binary classification\n",
        "bin_X = X[____]\n",
        "\n",
        "# Select the labels for binary classification, and convert it to 0 and 1\n",
        "bin_y = (y[___]==___).astype(np.uint8) "
      ],
      "metadata": {
        "id": "TEJwe-AvLvN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bin_indices = np.logical_or(y==1,y==2)\n",
        "\n",
        "bin_X = x[bin_indices]\n",
        "bin_y = (y[bin_indices]==True).astype(np.uint8)"
      ],
      "metadata": {
        "id": "V8tCbtMMld5l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a set of binary classification data we can use to train an algorithm.\n",
        "\n",
        "As we saw during our reading, we need to define three things in order to train our algorithm: \n",
        "> $\\cdot$ the type of algorithm we will train, \\\\\n",
        "> $\\cdot$ the cost function (which will tell us how close our prediction is to the truth), and \\\\\n",
        "> $\\cdot$ a method for updating the parameters in our model according to the value of the cost function (e.g., the gradient descent method). \n",
        "\n",
        "Let's begin by defining the type of algorithm we will use. We will train a logistic regression model to differentiate between two classes. A reminder of how the logistic regression algorithm works is given below.\n",
        "<br><br><br>\n",
        "The logistic regression algorithm will thus take an input $t$ that is a linear combination of the features:\n",
        "\n",
        "<a name=\"logit\"></a>\n",
        "\n",
        "<center> $t_{\\small{n}} = \\beta_{\\small{0}} + \\beta_{\\small{1}} \\cdot X_{1,n} + \\beta_{\\small{2}} \\cdot X_{2,n}$ </center>\n",
        "\n",
        "where \n",
        "* $n$ is the ID of the sample \n",
        "* $X_{\\small{0}}$ represents the petal length\n",
        "* $X_{\\small{1}}$ represents the petal width\n",
        "\n",
        "This input is then fed into the logistic function, $\\sigma$:\n",
        "\\begin{align} \n",
        "\\sigma: t\\mapsto \\dfrac{1}{1+e^ {-t}}\n",
        "\\end{align}\n",
        "\n",
        "Let's define the logistic function for later use."
      ],
      "metadata": {
        "id": "jvNBaOWZ9fXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q4) Define the logistic function**\n",
        "\n",
        "*Hint: numpy includes the exponential function in its library as [np.exp](https://numpy.org/doc/stable/reference/generated/numpy.exp.html).*"
      ],
      "metadata": {
        "id": "PzrhQ2E-zkDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic(in_val):\n",
        "    # Return the value of the logistic function\n",
        "    out_value = 1/(1 + np.exp(-in_val)) \n",
        "    return out_value"
      ],
      "metadata": {
        "id": "EdelvJlJzuE5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the logistic function has been defined, we can plot it (this will help us remember what it looks like!) Run the code below - you won't have to fill anything in for this one ðŸ˜€ But feel free to show the code and read through it - some of the functions used can be helpful to you down the line!"
      ],
      "metadata": {
        "id": "WqIkC1wZ0gAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this to plot the logistic function!\n",
        "# Let's generate an array of 20 points with values from -4 to +4 \n",
        "t = np.linspace(-4,4,20)\n",
        "\n",
        "# Initiate a figure and axes object using matplotlib\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Draw the X and Y axes\n",
        "ax.axvline(0, c='black', alpha=1)\n",
        "ax.axhline(0, c='black', alpha=1)\n",
        "\n",
        "# Draw the threshold line (y_val=0,5) and asymptote (y=1)\n",
        "[ax.axhline(y_val, c='black', alpha=0.5, linestyle='dotted') for y_val in (0.5,1)]\n",
        "\n",
        "# Scale things to make the graph look nicer\n",
        "plt.autoscale(axis='x', tight=True)\n",
        "\n",
        "# Plot the logistic function. X values from the t vector, y values from logistic(t)\n",
        "ax.plot(t, logistic(t));\n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$\\\\sigma\\\\  \\\\left(t\\\\right)$')\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "lgt9dI6b9Zwa",
        "outputId": "9e3b54ad-51b9-4aea-9ef8-f61e60977eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV5b3H8c8PAgESwhZ2CIRVFmWLICrVVi1Wa221i3VfKC516XJrva221nprr723u9pSwSlgUYuIIgpICxRECoQ1AYEYAmHJBiEr2Z/7xwkpzQ0kgeTMOcn3/XrlpTN5Er7G4Xwzc555xpxziIiIhJo2fgcQERGpiwpKRERCkgpKRERCkgpKRERCkgpKRERCUoTfAZpDbGysGzx4sN8xRJrEnj17ABg5cqTPSUSaRmJiYo5zrmd941pkQQ0ePJjNmzf7HUOkSVx55ZUArF692tccIk3FzA40ZJwu8YmISEhSQYmISEhSQYmISEhSQYmISEhSQYmISEhSQYmISEjypaDM7GEz22xmpWbm1TP222aWYWb5ZjbHzCKDFFNERHzk1xnUEeBZYM7ZBpnZdOAJ4CpgEDAE+EmzpxMREd/5UlDOuUXOucXAsXqG3gXMds4lO+dygZ8Cd9f3/Y8dO8a2bdsAqKysxPM8duzYAUB5eTme55GUlARASUkJnuexe/duAIqLi/E8r+bu/cLCQjzPIyUlBYC8vDw8zyM1NRWA3NxcPM8jLS0NgJycHDzPIz09HYCsrCw8z+Pw4cMAZGRk4HkeGRkZABw+fBjP88jKygIgPT0dz/PIyckBIC0tDc/zyM3NBSA1NRXP88jLywMgJSUFz/MoLCwEAqsOeJ5HcXExALt378bzPEpKSgBISkrC8zzKy8sB2LFjB57nUVlZCcC2bdvwPK/mZ5mYmMjcuXNrtjdt2sT8+fNrtjds2MCCBQtqttevX8/rr79es71u3ToWLlxYs71mzRoWLVpUs71q1SoWL15cs71y5UqWLFlSs71ixQqWLl1as71s2TKWLVtWs7106VJWrFhRs71kyRJWrlxZs7148WJWrVpVs71o0SLWrFlTs71w4ULWrVtXs/3666+zfv36mu0FCxawYcOGmu358+ezadOmmu25c+eSmJhYs+15XpMfeydPngR07OnYC/6x11yvew0V6u9BjQG2n7a9HehtZj1qDzSzmdWXDTcXFBQELaCIiDQP8/OJumb2LDDAOXf3GT7/CfBN59yy6u12QBkQ75xLO9P3TUhIcFrqSFoKLXUkLY2ZJTrnEuobF+pr8RUCMadtn/p3nSKJiISRisoqcgrLyMwvafDXhHpBJQPjgDeqt8cBmc65+t67EhGRIKiorOJYUaB4MvNLySqo/md+CVkFpTX7jxWV0tgLdr4UlJlFVP/ZbYG2ZtYBqHDOVdQaOhfwzOxVAjP/ngS8YGYVEWmtKiqrOHziJKk5RRw9UUJmfglZBSVk5ZeSWV1ExwpLqapVPGbQIyqS3jGR9I7pwIX9u9ArpgO9YyLp1bkDn/3vhv35fp1BPQn8+LTt24GfmNkcYBcw2jl30Dm3zMyeB1YBHYE3a32diIicB+cc2YWl7M8uYn9O4CM1p4jU7EIOHi+mvPJf7XOqeHp1DpTP2H5d6NU5srp8OlTv70BsdHsi2p7/HDxfCso59zTw9Bk+HV1r7C+BXzZzJBGRFq2gpJy0nGJScwr/VUTVpVRY+q+LV+0j2jC4RyeG9YrmmtF9GNIzivjYKAZ060hsdCTtmqB4GirU34MSEZFGKK2oZNeRfLYePMG+rAJSswNnRNkFpTVjzKB/147Ex0Zx88T+xMdGMaRnNPGxUfTr2pG2bczH/4J/UUGJiIQp5xyHT5xk68ETgY/0XJIP51NWWQVA96j2DImN4soRPYnvGcWQ6iKK696JDu3a+py+fiooEZEwUVxWwc5DeWxNP8GWA7lsTT9Rc2bUoV0bLurflXsuG8yEuK6MH9iNPl06+Jz4/KigRERCkHOOtGPF1UWUy9aDJ/g4o4DK6ilzg3t04vJhsUyI68rEuG6M7NM5qO8PBYMKSkQkBFRWOTanHeef+4+z9WDg7OhEcWDdwujICMYP7MpDVw6tOTvqHtXe58TNTwUlIuKTkvJKPkzJYXlyBit3Z3G8qAwzGN4rmumj+wTOjgZ1Y2jP6JCZuBBMKigRkSAqKCln1Z5slidlsHpPFkVllXSOjOAzo3px7Zg+XDY8lpgO7fyOGRJUUCIizSynsJQPdmWyPDmD9SnHKKusomfnSG6c0J/pY/owdUgP2ke0rPePmoIKSkSkGaQfL2Z5cgbLkzPYfCAX5yCueyfuvmww08f0ZsLAbrRphZftGkMFJSLSBJxz7MksYHlS4Exp19F8AEb1jeGxq4YzfUwfLujTGTOVUkOpoEREzkPS4TyWbD/C8uQM0o4VYwaT4rrxw+tGMX1MH+J6dPI7YthSQYmINFJllWPl7kxmr93PxrTjtGtrTB0ay8xPDeXq0b3o1Tm8b5ANFSooEZEGKi6r4M3EQ8xet5+0Y8X079qRJ68fxVcSBtKlo2beNTUVlIhIPbLyS/jzR2m8+s+DnCguZ9zArvx++kiuHdOnSR4rIXVTQYmInMHuo/m8vHY/72w/TEWVY/roPsyYFs+kQd002SEIVFAiIqdxzrF6bzaz1+5nXUoOndq35bYpg7jnssEM6hHld7xWRQUlIkJg2aHFWw8ze91+9mUV0jsmku9fewG3To6jSye9v+QHFZSItGrHCkuZt+EA8z46wLGiMkb3jeFXXxvH9Rf20+oOPlNBiUirlJJVyOx1+1m05RClFVV85oJezJgWz9QhPfT+UohQQYlIq5J+vJj/WrqbZckZREa04aaJA7jv8niG9Yr2O5rUooISkVahrKKKP61N5Xd/34dhPHbVcO6cOoge0ZF+R5MzUEGJSIu3PiWHp95O4pPsIq4d04cf3TCafl07+h1L6qGCEpEWKyu/hP96bzdvbztCXPdOvHL3xXz6gl5+x5IGUkGJSItTUVnFvA0H+OWKvZRWVPHoVcN56MqhdGjX1u9o0ggqKBFpUbYczOXJt5LYdTSfacNjeebGscTH6gbbcKSCEpEWIbeojOeXf8yCjen0ienAi7dN5HNj+2jKeBhTQYlIWKuqcixMPMRz7+8mv6SCb0yL57GrRxAdqZe3cKf/gyIStnYdyeept5NIPJBLwqBuPPulsVzQJ8bvWNJEVFAiEnYKSsr51Qf7+PNHaXTp2I5ffPkibp44gDZtdDmvJVFBiUjYcM6xdOdRfvruLrIKSvn65Dgenz6Srp3a+x1NmoEKSkTCwv6cIn70dhJr9+Uwtn8Mf7wjgfEDu/odS5qRL0v1mll3M3vLzIrM7ICZ3XqGcZFm9gczyzSz42a2xMz6BzuviPjrvZ1Huf63a9l28ATP3DiGt795ucqpFfDrDOoFoAzoDYwHlprZdudccq1xjwFTgYuAPGAW8DvgpiBmFRGfVFY5frF8D39Y8wkT4rry4m0T6dtFSxS1FkEvKDOLAm4GxjrnCoF1ZvYOcAfwRK3h8cBy51xm9de+DvwymHlFxB+5RWU8smAr61JyuG1KHD+6YTSREVoJojXx4wxqBFDhnNt72r7twBV1jJ0N/MbM+gEngNuA9+v6pmY2E5gJEBcX16SBRSS4kg7ncf+8RLILS3n+5ov46sUD/Y4kPvDjPahoIL/Wvjygcx1j9wHpwOHqrxkFPFPXN3XOzXLOJTjnEnr27NmEcUUkmN5MPMTNL63HOcdf75+qcmrF/DiDKgRq30kXAxTUMfYFIBLoARQBjxM4g5rSnAFFJPjKKqp4duku5n50gKlDevC7WycQq2c1tWp+nEHtBSLMbPhp+8YBtSdIQGACheecO+6cKyUwQWKymcUGIaeIBElWfgm3vbyBuR8d4BvT4pl332SVkwT/DMo5V2Rmi4BnzGwGgRK6Ebi0juGbgDvNbDVQDDwEHHHO5QQrr4g0r8QDx3lw/hYKSir47dcn8IVx/fyOJCHCl/ugCBRNRyALWAA86JxLNrNpZlZ42rj/AEoIvBeVDVwHfCnYYUWk6TnnmLfhALfM2kDH9m1565uXqpzk3/hyH5Rz7jjwxTr2ryUwieLU9jECM/dEpAUpKa/kycVJLEw8xKdH9uTXX5tAl07t/I4lIUZLHYlIUB0+cZIH5iWy83Aej141nG9dNVyLvEqdVFAiEjQfpuTwyIKtlFdU8fKdCVw9urffkSSEqaBEpNk555j1j1T+e9nHDO0ZzR/vmMSQntH1f6G0aiooEWlWRaUVPP7mDpbuOMp1F/bh+S+P09NupUF0lIhIs9mfU8T98zaTklXIE5+7gPs/NQQzvd8kDaOCEpFmkXwkjztmb8Q5x9x7p3D5cN1fL42jghKRJrc9/QR3zP4n0ZERvPqNS4iPjfI7koQhFZSINKnNace5+5VNdItqx19mXMLA7p38jiRhSgUlIk1m/Sc5zPjzZvrEdODVb0zRwwXlvPi11JGItDBr9mZzzyub6N+1I6/df4nKSc6bzqBE5Lx9sCuTb766hWG9opl332R6aCVyaQIqKBE5L+/tPMqjC7Yypl8Mc++dojX1pMnoEp+InLPFWw/z8F+2MH5gV+bPUDlJ09IZlIick9c3HeSJRTu5JL4HL9+VQJRWh5AmpiNKRBpt3kdpPPV2Mp8a0ZNZd0yiQ7u2fkeSFkgFJSKN8vLaVJ5dupurR/XihdsmEhmhcpLmoYISkQb7/d/38T8r9nLdhX349dcm0D5Cb2NL81FBiUi9nHP88oO9/O7vKXxxfD/+5yvjiGircpLmpYISkbNyzvHc+x8z6x+pfC1hID+76ULa6gm4EgQqKBE5o6oqx0+WJPPnjw5wxyWD+MkXxujx7BI0KigRqVNVleMHb+3ktU3pzLg8nh9eP0rPcpKgUkGJyP9TUVnF4wt3sGjrYR7+9DC++9kRKicJOhWUiPyb8soqvvXaNpbuPMp3rxnBI1cN9zuStFIqKBGpUVnleHTBVt5PyuAH113AzE8N9TuStGIqKBEBArP1nly8k/eTMnjy+lHMmDbE70jSyulGBhEB4Jcf7GXBxnS++emhKicJCSooEcH7cD+/+3sKt1w8kP/47Ei/44gAKiiRVm/J9iP85N1dfHZ0b5794ljN1pOQoYISacXW7svmO29s4+JB3fnt1ydo+SIJKToaRVqpHYdOcP+8RIb2jOZPdyXokRkSclRQIq1QanYhd7+yie5R7Zl772S6dNSTcCX0qKBEWpnM/BLumL0RA+bdN4VeMR38jiRSJ18Kysy6m9lbZlZkZgfM7NazjJ1oZv8ws0IzyzSzx4KZVaQlyTtZzl1zNnKiuAzvnsnEx0b5HUnkjPy6UfcFoAzoDYwHlprZdudc8umDzCwWWAZ8G1gItAcGBDmrSItQUl7JjD9v4pPsQl65ezIXDujidySRswr6GZSZRQE3A0855wqdc+uAd4A76hj+HWC5c+5V51ypc67AObc7mHlFWoKKyioe/stWNh/I5VdfG8/lw2P9jiRSLz8u8Y0AKpxze0/btx0YU8fYS4DjZrbezLLMbImZxdX1Tc1sppltNrPN2dnZzRBbJDw5F3hsxsrdmfzkC2P4/EX9/I4k0iB+FFQ0kF9rXx7QuY6xA4C7gMeAOGA/sKCub+qcm+WcS3DOJfTs2bMJ44qEt18s38Mbmw/x6FXDuXPqYL/jiDSYH+9BFQIxtfbFAAV1jD0JvOWc2wRgZj8Bcsysi3Mur3ljioS/Oev28+LqT7h1ShzfvlqPzZDw4scZ1F4gwsxO/9syDkiuY+wOwJ227eoYIyJ1eHvbYZ55dxefG9uHn96oJYwk/AS9oJxzRcAi4BkzizKzy4AbgXl1DH8F+JKZjTezdsBTwDqdPYmc3Zq92Xz3je1cMqQ7v/raeNq2UTlJ+PHrRt2HgI5AFoH3lB50ziWb2TQzKzw1yDn3d+AHwNLqscOAM94zJSKwLf0ED85PZETvzvzpTi1hJOHLl/ugnHPHgS/WsX8tgUkUp+97CXgpSNFEwlpKViH3vLKR2OhIvHsvpnMHLWEk4UtLHYm0EEfzTnLXnI20bdOGefdNpldnLWEk4a3RZ1BmFgn0I3CJLts5p5uORHx2oriMu+ZsJO9kOa/NvIRBPbSEkYS/Bp1BmVlnM3vQzP5B4J6lFCAJyDCzg2b2JzO7uDmDikjdAksYbSYtp5hZd05ibH8tYSQtQ70FZWbfAdKAe4EPCMy4G09gRYipwNMEzsQ+MLNltaaPi0gzqqpyfPeN7SQeDCxhdOlQLWEkLUdDLvFdAlzhnEs6w+c3AnPM7AHgPuAKYF8T5RORs/jvZR+zdOdRfnjdKK6/qK/fcUSaVL0F5Zz7akO+kXOuFHjxvBOJSIPM23CAP/4jlTunDmLGtHi/44g0uUbN4jOzJDPTBW4Rn/1tdyY/fjuJq0f14sc3jNEqEdIiNXaa+WggsvZOM+tiZi80TSQROZsdh07w8F+2MrZ/F3779QlaJUJarIbO4nu/eqFWBwysY0gn4P6mDCYi/1/68WLu9TbTPao9L9+VQKf2fj1zVKT5NfTo3klg8oMBG82sgMAznLYSWND1AuBosyQUEQDyisu5x9tEWUUlr82cohtxpcVrUEE55x4HMLNSAlPL+xGYaj4euL76+zzeTBlFWj1nbbl//mYOHCti3n1TGNarrsenibQsjb0+EO2cKwe2AO82Qx4RqcUBOUOu5UDqcX5zy3guGdLD70giQdGQG3Vr5q9Wl9PZxpqZ1fUelYicoxMDLqeo52i+N30kN47v73cckaBpyCSJj8xstplNPdMAM+tmZg8CuwisNOGrY8eOsW3bNgAqKyvxPI8dO3YAUF5ejud5JCUF7jsuKSnB8zx2794NQHFxMZ7nsWfPHgAKCwvxPI+UlBQA8vLy8DyP1NRUAHJzc/E8j7S0NABycnLwPI/09HQAsrKy8DyPw4cPA5CRkYHneWRkZABw+PBhPM8jKysLgPT0dDzPIycnB4C0tDQ8zyM3NxeA1NRUPM8jLy/wSKyUlBQ8z6OwMPCUkj179uB5HsXFxQDs3r0bz/MoKSkBICkpCc/zKC8P/K6xY8cOPM+jsrISgG3btuF5Xs3PMjExkblz59Zsb9q0ifnz59dsb9iwgQULFtRsr1+/ntdff71me926dSxcuLBme82aNSxatKhme9WqVSxevLhme+XKlSxZsqRme8WKFSxdurRme9myZSxbtqxme+nSpaxYsaJme8mSJaxcubJme/Hixaxatapme9GiRaxZs6Zme+HChaxbt65m+/XXX2f9+vU12wsWLGDDhg012/Pnz2fTpk0123PnziUxMbFm2/O8Jj32Hvn1a+QNmEp05nZumxCrY0/HXs12cx97zfm611ANucR3AfBDYKmZVQGJwBGgBOhGYOr5KAIrSnzLObe8UQlEpE4ffnKcdzOiaX8shR5pKzH7pt+RRILKnGvYU9TNrCOBCRGXA4MIrGaeQ2Am3/KzLIUUdAkJCW7z5s1+xxA5Z7uO5POVP6wnrkcURW8/Q5uqclavXu13LJEmYWaJzrmE+sY1eJKEc+4ksLD6Q0SaydG8k9zrbSKmYzteuftibnnrrG/9irRYustPJITkl5RzzyubKCyt4K8PTKVPF93rJK2XnqgrEiLKK6v45qtbSMkq5KXbJzKqb4zfkUR8pTMokRDgnOMHi3aydl8Oz3/5IqYN7+l3JBHfNXY18yHNFUSkNfvd31P4a+IhHr1qOF9N0K2EItD4S3z7zOyWZkki0kq9mXiIX36wl5sm9ufbV+uB1CKnNLagDHjMzPaY2cdmNs/MrmmOYCKtwfqUHL7/5g4uHdqDn990kZ7rJHKac5kkEQe8CcwDooG3zexlM9OEC5FG2JtZwP3zExnSM4qXbp9E+wj9FRI53blMkrjVOVezXoeZDSOwcOz3geeaKphIS3bkxEnumrORDu3aMufui+nSsZ3fkURCTmN/ZcsBsk7f4ZxLAR4DZjRVKJGWLLeojDvnbKSwpALvnosZ0K2T35FEQlJjC2obMLOO/QcALbMsUo/isgru/fMmDh4vZtadCYzp18XvSCIhq7GX+J4EVplZf+BFAk/T7Qg8BaQ2cTaRFqW8soqHXt3C9vQTvHjbRKYO1XOdRM6mUQXlnNtoZlOA3wAf8K8zsJPAl5s4m0iLUVXl+P7CHazek81zN13ItWP7+h1JJOQ1epJE9arlV5lZD2AS0Bb4p3PueFOHE2kJnHP87L3dLNp6mO9eM4KvT47zO5JIWDjnpY6cc8eAFfUOFGnl/viPVF5et5+7pg7i4c8M8zuOSNjw5cYLM+tuZm+ZWZGZHTCzW+sZ397MdpvZoWBlFGkKf92czs/f/5jPX9SXH98wRjfiijSCX4vFvgCUAb2B8QSe1rvdOZd8hvHfA7KBzkHKJ3LeVu7K5IlFO7l8WCz/+9VxtGmjchJpjKCfQZlZFHAz8JRzrtA5tw54B7jjDOPjgdvRTcASRjanHeebf9nCmH4x/OGOSURGtPU7kkjY8eMS3wigwjm397R924ExZxj/O+AHBGYKnpGZzTSzzWa2OTs7u2mSipyDPRkF3Ottol/Xjrxy98VER+qpNiLnwo+Cigbya+3Lo47Ld2b2JaCtc+6t+r6pc26Wcy7BOZfQs6eepSP+OJRbzJ1z/kmHdm2Ze+9kekRH+h1JJGz58atdIVD7UaExQMHpO6ovBT4PXBekXCLn5Xj1EkbFZZW8cf9UBnbXEkYi58OPgtoLRJjZcOfcvup944DaEySGA4OBtdUzn9oDXcwsA7jEOZcWnLgi9SsqreCeVzZyOPck8+6bose1izSBoBeUc67IzBYBz5jZDAKz+G4ELq01NAk4/dGilwK/ByYSmNEnEhLKKqp4YH4iSUfy+cPtk5gc393vSCItgl8PoHmIwBp+WcAC4EHnXLKZTTOzQgDnXIVzLuPUB3AcqKrervQpt8i/qapy/Mdft7N2Xw7PfelCrhnd2+9IIi2GL9OLqpdF+mId+9cSmERR19esBgY0bzKRhnPO8dOlu3hn+xEev3YkX714YP1fJCINpkd4ipyjF1d/wisfpnHvZfE8eMVQv+OItDgqKJFz8Pqmg/xi+R6+OL4fT14/SksYiTQDFZRII61IzuA/F+3kUyN68vyXtYSRSHNRQYk0wj9Tj/HIgq1cOKArL902kfYR+isk0lz0t0ukgT765Bj3eJsY0C2whFGUljASaVYqKJEG+MfebO5+ZSP9u3ZkwTcuoXtUe78jibR4+hVQpB5/253Jg/O3MLRXNPPv0/p6IsGighI5i/d3HuWRBVsZ3S+GufdOpmsnnTmJBIsKSuQM3t52mO+8sZ3xA7vyyj0XE9Ohnd+RRFoVvQclUoc3Nqfzrde3cfHgbsy9d7LKScQHOoMSqWX+hgM8uTiJacNjmXVHAh3b62m4In5QQYmcZva6/fz03V1cPaoXv791Ih3aqZxE/KKCEqn2wqoUfrF8D58b24ff3DJBN+GK+EwFJa2ec45frdzHb/+2jxvH9+N/vzKOiLYqJxG/qaCkVXPO8fNlH/PHNal8NWEAz910EW21tp5ISFBBSavlnOMnS3bhrU/j9kvieOYLY7Xwq0gIUUFJq1RV5fjh4iQWbDzIfZfH65EZIiFIBSWtTmWV4/GFO3hzyyEeunIo35s+UuUkEoJUUNKqlFdW8Z03trNk+xG+c80IHvnMMJWTSIhSQUmrUVpRyaMLtrI8OZP//NwF3K/HtIuENBWUtAol5ZU8OD+RVXuyefqG0dx9WbzfkUSkHiooafGKyyqYOTeRDz/J4WdfupBbp8T5HUlEGkAFJS3a0byTPDB/CzsPneAXXx7HlycN8DuSiDSQCkparA2px3j4L1s4WVbJS7dPYvqYPn5HEpFGUEFJi+OcY86Hafzsvd0M6tGJ12ZOZVivaL9jiUgjqaCkRTlZVskTi3bw9rYjfHZ0b/73q+PorGc5iYQlFZS0GAePFTNz3mb2ZBbwvekjefCKoVq6SCSMqaCkRVi9J4vHXtsGgHfPZK4Y0dPnRCJyvlRQEtaqqhwvrk7hfz/YywV9Yvjj7ZOI69HJ71gi0gRUUBK2CkrK+e4b21mxK5Mbx/fj5zddpMezi7QgKigJSylZBcycl8iBY8X86POjueeywVpTT6SFUUFJ2FmWlMF339hGx/ZteXXGFC4Z0sPvSCLSDHx5rrWZdTezt8ysyMwOmNmtZxj3PTNLMrMCM9tvZt8LdlYJHZVVjueXfcwD8xMZ3rszSx65XOUk0oL5dQb1AlAG9AbGA0vNbLtzLrnWOAPuBHYAQ4EVZpbunHstqGnFd7lFZTz62lbW7svh65PjePoLo4mM0PtNIi1Z0AvKzKKAm4GxzrlCYJ2ZvQPcATxx+ljn3POnbe4xs7eBywAVVCuSdDiPB+YnkpVfys9vupBbJmuxV5HWwI9LfCOACufc3tP2bQfGnO2LLPAO+DSg9lnWqc/PNLPNZrY5Ozu7ycKKv97aeoibX1pPZZXjjQemqpxEWhE/LvFFA/m19uUBnev5uqcJFOordX3SOTcLmAWQkJDgzi+i+K28sor/Wrobb30aU+K788JtE4mNjvQ7logEkR8FVQjE1NoXAxSc6QvM7GEC70VNc86VNmM2CQHb00/w5OIkdh7O477L43nicxfQrq0v83lExEd+FNReIMLMhjvn9lXvG8eZL93dS+C9qU855w4FKaP4IK+4nF+s+JhX/3mQ2OhIXrxtItdd2NfvWCLik6AXlHOuyMwWAc+Y2QwCs/huBC6tPdbMbgN+BnzaOZca3KQSLM45Fm05zM/e201ucRl3XzqYb18zghitQi7Sqvk1zfwhYA6QBRwDHnTOJZvZNOB959yph/c8C/QANp22SsB859wDwQ4szWNPRgFPLU5iY9pxJsR1Ze59kxnTr4vfsUQkBPhSUM6548AX69i/lsAkilPb8cHMJcFTVFrBb/62jznr9hPdIYL/vvlCvjJpoB6PISI1tNSRBJVzjmVJGTzz7i6O5pVwy8UDefzaC+ge1d7vaCISYlRQEjRpOUX8+BEktDwAAAtGSURBVJ1k1uzNZlTfGH5/60QmDermdywRCVEqKGl2JeWV/GHNJ7y4+hPat23Djz4/mjunDiJCU8dF5CxUUNKsVu/J4sfvJHPgWDE3jOvHk9ePondMB79jiUgYUEFJsziad5KfvruL93ZmMKRnFK/OmMJlw2L9jiUiYUQFJU2qvLKKVz7cz69X7qOyyvG96SOZMS1eK4+LSKOpoKRJOOdYszeb5977mD2ZBVw9qhc/vmEMA7t38juaiIQpFZScl9KKSt7edoTZa/ezJ7OA/l078qc7E7hmdG+/o4lImFNByTk5XlTGqxsO8OePDpBTWMoFfTrzP18Zxw3j+upynog0CRWUNMon2YXMWbefN7ccoqS8iitG9OQb04Zw2bAenLYclYjIeVNBSb2cc2xIPc7sdams3J1F+4g2fGl8f+6bFs+I3vU9xktE5NyooOSMyiurWLrjKC+vSyXpcD7do9rz6FXDueOSQfTsrIcHikjzUkHJ/5N3spwFGw/ifZhGRn4JQ3tG8bMvXchNE/vToZ3eXxKR4FBBSY3048XMXrefNzanU1xWyaVDe/Czm8Zy5YheWmVcRIJOBSUkHsjl5bWpLE/OoI0ZXxjXj/umxeu5TCLiKxVUK+ScY9fRfJYnZbA8OZM9mQXEdIjg/iuGctfUwfTporXyRMR/KqhWorLKkXggl+XJGSxPzuBQ7knaGCQM7s5PbxzDTRMHEBWpw0FEQodekVqw0opK1qccY3lyBit3Z5JTWEb7tm24fHgsj3xmGFeP6k2PaM3GE5HQpIJqYQpLK1i9J4vlyZms+jiLwtIKoiMjuHJkT64d24crR/YiWmdKIhIG9ErVAhwrLGXl7kyWJ2eyLiWHsooqekS15/MX9WX6mD5cOqyHlh8SkbCjggpTh0+crJ7kkMGmtONUOejftSO3TxnEtWP7MGlQN9pqariIhDEVVBioqKxib2YhW9Nz2XrwBFsP5vJJdhEAI3t35uFPD+OzY/owpl+M1sMTkRZDBRWCsgtK2Xowl63pgTLacSiP4rJKAHpEtWdCXFe+dvFAPju6D4Njo3xOKyLSPFRQPiurqGLX0fxAIR08wdb0XNKPnwQgoo0xpl8MX00YyIS4rkwY2I2B3TvqLElEWgUVVBA55ziaV8LWgyfYcjCXrQdzSTqST1lFFQB9u3RgQlxX7rxkMBPiujK2fxetfScirZYKqhlUVTmO5pewP7uI/TmFpOYUsT+niN1H88nMLwUgMqINF/bvwl1TBzExrhvj47rSt0tHn5OLiIQOFdR5yC0qqymf/TmF7M8pIjW7iLRjRZSUV9WM69S+LfGxUUwd0oMJcd2YENeVC/rE0D6ijY/pRURCmwqqHiXllaQdCxTPqQI6VUa5xeU149q2MeK6d2JIbBSXD4slvmcU8bFRDO0ZTa/OkXrfSESkkVptQVVUVpFTWEZmfgmZ+SVkFZSSlV9CZn4pWQX/+mdOYdm/fV3vmEjiY6O4dmxfhlaXUHxsFAO7d6JdW50RiYg0lRZZUOWVjh2HTpCZX1qrfAL/nplfyrGiUpz7968zg9joSHp1jqRPlw5cNKALfbt0JL5nFENioxgcG6VlgkREgqRFvtp+nJHPF37/Yc22GfSIiqR3TCS9YzpwYf8u9IrpQK/Oge3eMZH06tyB2Oj2ROgsSEQkJPhSUGbWHZgNfBbIAf7TOfeXOsYZ8HNgRvWul4EnnKt97vPv+nftyB/vTKgpIBWPiEj48esM6gWgDOgNjAeWmtl251xyrXEzgS8C4wAHfADsB/5wtm/ePao914zu3eShRUQkeKyek5Gm/wPNooBcYKxzbm/1vnnAYefcE7XGrgc859ys6u37gG845y4525/RuXNnN2nSpGbJLxJs27ZtA2D8+PE+JxFpGmvWrEl0ziXUN86P614jgIpT5VRtOzCmjrFjqj9X3zjMbKaZbTazzeXl5XUNERGRMOLHJb5oIL/Wvjyg8xnG5tUaF21mVvt9qOqzrFkACQkJbvXq1U0WWMRPV155JQA6pqWlaOh9oX6cQRUCMbX2xQAFDRgbAxTWN0lCRETCnx8FtReIMLPhp+0bB9SeIEH1vnENGCciIi1M0AvKOVcELAKeMbMoM7sMuBGYV8fwucB3zKy/mfUDvgt4QQsrIiK+8evmoIeAjkAWsAB40DmXbGbTzKzwtHF/BJYAO4EkYGn1PhERaeF8uQ/KOXecwP1NtfevJTAx4tS2Ax6v/hARkVZEyyuIiEhIUkGJiEhIUkGJiEhIUkGJiEhICvpafMFgZgXAHr9zNFIsgZXdw4kyB0845lbm4AjHzCOdc3WtHvRvWuTzoIA9DVmIMJSY2WZlbn7hmBnCM7cyB0e4Zm7IOF3iExGRkKSCEhGRkNRSC2qW3wHOgTIHRzhmhvDMrczB0WIzt8hJEiIiEv5a6hmUiIiEORWUiIiEJBWUiIiEpFZRUGY23MxKzGy+31nqY2bzzeyomeWb2V4zm+F3prMxs0gzm21mB8yswMy2mdnn/M5VHzN72Mw2m1mpmXl+5zkTM+tuZm+ZWVH1z/hWvzOdTbj8XE8XxsdwWL1WnK6hr8mtoqCAF4BNfodooOeAwc65GOALwLNmNsnnTGcTAaQDVwBdgCeBN8xssI+ZGuII8Cwwx+8g9XgBKAN6A7cBL5nZGH8jnVW4/FxPF67HcLi9VpyuQa/JLb6gzOwW4ATwN7+zNIRzLtk5V3pqs/pjqI+Rzso5V+Sce9o5l+acq3LOvQvsB0L6L4pzbpFzbjFwzO8sZ2JmUcDNwFPOuULn3DrgHeAOf5OdWTj8XGsL42M4rF4rTmnMa3KLLigziwGeAb7jd5bGMLMXzawY+Bg4Crznc6QGM7PewAgg2e8sLcAIoMI5t/e0fduBUD6DCnvhdAyH22tFY1+TW3RBAT8FZjvnDvkdpDGccw8BnYFpwCKg9OxfERrMrB3wKvBn59zHfudpAaKB/Fr78ggcG9IMwu0YDsPXika9JodtQZnZajNzZ/hYZ2bjgauBX/md9ZT6Mp8+1jlXWX1JZwDwoD+JG57ZzNoA8wi8X/KwX3mrszT45xziCoGYWvtigAIfsrR4oXQMN0aovFbU51xek8N2NXPn3JVn+7yZfQsYDBw0Mwj8NtrWzEY75yY2e8A61Jf5DCLw8bpyQzJb4Ac8m8Ab+dc558qbO9fZnOPPORTtBSLMbLhzbl/1vnGEwaWncBNqx/A58vW1ogGupJGvyWF7BtUAswj8zxpf/fEHYCkw3c9QZ2NmvczsFjOLNrO2ZjYd+DqhP8HjJWAUcINz7qTfYRrCzCLMrAPQlsBfkg5mFlK/sDnnighctnnGzKLM7DLgRgK/5YekcPi5nkFYHcNh+lrR+Ndk51yr+ACeBub7naOejD2BNQRmuOQDO4Fv+J2rnsyDCMweKiFwSerUx21+Z2vA8eBqfTztd646cnYHFgNFwEHgVr8ztYSfa63MYXcMh+NrxRmOlbO+JmuxWBERCUkt+RKfiIiEMRWUiIiEJBWUiIiEJBWUiIiEJBWUiIiEJBWUiIiEJBWUiIiEJBWUSIgzs1+Y2XK/c4gEmwpKJPRNBjb6HUIk2LSShEiIMrP2BJbcaXfa7t3OudE+RRIJKp1BiYSuCmBq9b9PAfoCl/kXRyS4wmGVYZFWyTlXZWZ9CTz/aZPT5Q5pZXQGJRLaJgDbVU7SGqmgRELbeGCr3yFE/KCCEglt44AdfocQ8YMKSiS0RQAXmFk/M+vqdxiRYFJBiYS2HwK3AIeA53zOIhJUug9KRERCks6gREQkJKmgREQkJKmgREQkJKmgREQkJKmgREQkJKmgREQkJKmgREQkJKmgREQkJP0fgaWxXfT0wSAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the logistic function, we define inputs resulting in $\\sigma\\geq0.5$ as belonging to the ***one*** class, and any value below that is considered to belong to the ***zero*** class.\n",
        "\n",
        "We now have a function which lets us map the value of the petal length and width to the class to which the observation belongs (i.e., whether the length and width correspond to Iris Versicolor or Iris Virginica). However, there is a parameter vector **$\\theta$** with a number of parameters that we do not have a value for: <br> $\\theta = [ \\beta_{\\small{0}}, \\beta_{\\small{1}}$, $\\beta_{\\small{2}} ]$"
      ],
      "metadata": {
        "id": "0Ll1PKpjxqLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q5) Set up an array of random numbers between 0 and 1 representing the $\\theta$ vector.**\n",
        "\n",
        "*Hint 1:  Use `rnd_gen`! If you're not sure how to use it, consult the `default_rng` documentation [at this link](https://numpy.org/doc/stable/reference/random/generator.html). For instance, you may use the `random` method of `rnd_gen`.*\n",
        "\n",
        "*Hint 2: The theta array should have 3 elements in it!*"
      ],
      "metadata": {
        "id": "O_lT4EaK2ICa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint 3: Random Value Array Snippet \n",
        "rnd_gen.random((___,)) # length of array"
      ],
      "metadata": {
        "id": "4ULzWzd750RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta_vector = rnd_gen.random((3,))"
      ],
      "metadata": {
        "id": "-Vk05y1C2VBs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to determine whether a set of $\\beta$ values is better than the other, we need to quantify well the values are able to predict the class. This is where the cost function comes in.\n",
        "\n",
        "The cost function, $c$, will return a value close to zero when the prediction, $\\hat{p}$, is correct and a large value when it is wrong. In a binary classification problem, we can use the log loss function. For a single prediction and truth value, it is given by:\n",
        "\\begin{align}\n",
        "        \\text{c}(\\hat{p},y) = \\left\\{\n",
        "        \\begin{array}{cl}\n",
        "        -\\log(\\hat{p})& \\text{if}\\; y=1\\\\\n",
        "        -\\log(1-\\hat{p}) & \\text{if}\\; y=0\n",
        "        \\end{array}\n",
        "        \\right.\n",
        "    \\end{align}\n",
        "\n",
        "However, we want to apply the cost function to an n-dimensional set of predictions and truth values. Thankfully, we can find the average value of the log loss function $J$ for an an-dimensional set of $\\hat{y}$ & $y$ as follows:\n",
        "\n",
        "\\begin{align}\n",
        "        \\text{J}(\\mathbf{\\hat{p}},y) = - \\dfrac{1}{n} \\sum_{i=1}^{n} \n",
        "        \\left[ y_i\\cdot \\log\\left( \\hat{p}_i \\right) \\right] + \n",
        "        \\left[ \\left( 1 - y_i \\right) \\cdot \\log\\left( 1-\\hat{p}_i \\right) \\right]\n",
        "    \\end{align}\n",
        "\n",
        "We now have a formula that can be used to calculate the average cost over the training set of data.\n",
        "\n",
        "Now let's code ðŸ’»\n"
      ],
      "metadata": {
        "id": "s8KM_CeF2Ven"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q6) Define a log_loss function that takes in an arbitrarily large set of prediction and truths**\n",
        "\n",
        "*Hint 1: You need to encode the function $J$ above, for which Numpy's functions may be quite convenient (e.g., [`log`](https://numpy.org/doc/stable/reference/generated/numpy.log.html), [`mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html), etc.)*\n",
        "\n",
        "*Hint 2: Asserting the dimensions of the vector is a good way to check that your function is working correctly. [Here's a tutorial on how to use `assert`](https://swcarpentry.github.io/python-novice-inflammation/10-defensive/index.html#assertions). For instance, to assert that two vectors `X` and `y` have the same dimension, you may use:*\n",
        "```\n",
        "assert X.shape==y.shape\n",
        "```"
      ],
      "metadata": {
        "id": "XBLxwlSWMoo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint 3: Example code snippet\n",
        "J_vector  = -(y * np.log(y_hat + epsilon) + (1-y) * np.log(1-y_hat))\n",
        "J.mean()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Jmnzz4h_Cq01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(p_hat, y, epsilon=1e-7):\n",
        "  \n",
        "  # Begin by calculating the two possibilities for the cost function, i.e.\n",
        "  # 1: -log(p_hat + epsilon), and 2: -log(1- p_hat). We added an epsilon term \n",
        "  # to -log(p_hat) because we can run into mathematical problems if p_hat = 0.\n",
        "  term_1 = -np.___( _____ + _____ )\n",
        "  term_2 = -np.___( 1 - ____ )\n",
        "  \n",
        "  # We can almost calculate J! We'll need to 1) multiply term_1 by y, and \n",
        "  # 2) multiply term_2 by (1-y). We then add the new terms together.\n",
        "  # Calculate the value of the cost function (i.e., what's inside the brackets)\n",
        "  inside_brackets = (__) * term_1 + ( ___ + ___ ) * term_2\n",
        "\n",
        "  #Verify the shape of inside_brackets. \n",
        "  print(f'The size of the term inside the brackets is {inside_brackets.shape}')\n",
        "\n",
        "  # You should have a cost value for each one of your predictions. We won't\n",
        "  # use the individual values, though. We'll aggregate the information from\n",
        "  # all our predictions by calculating the mean! (i.e., 1/n_terms * terms_sum)\n",
        "  # This single value is J\n",
        "  J = _____.mean()\n",
        "\n",
        "  return J"
      ],
      "metadata": {
        "id": "H5fDeL36EauO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(p_hat, y, epsilon=1e-7):\n",
        "  term_1 = -np.log(p_hat + epsilon)\n",
        "  term_2 = -np.log(1 - p_hat)\n",
        "  inside_brackets = y * term_1 + (1 - y)* term_2\n",
        "  print(f'The size of the term inside the brackets is {inside_brackets.shape}')\n",
        "  J = inside_brackets.mean()\n",
        "  return J "
      ],
      "metadata": {
        "id": "X9G0obwSwawa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a way of quantifying how good our predictions are. The final thing needed for us to train our algorithm is figuring out a way to update the parameters in a way that improves the average quality of our predictions. \n",
        "\n",
        "<br><br>**Warning**: we'll go into a bit of math below <br><br>\n",
        "\n",
        "Let's look at the change in a single parameter within $\\theta$: $\\beta_1$ (given $X_{1,i} = X_1$, $\\;\\hat{p}_{i} = \\hat{p}$, $\\;y_{i} = y$). If we want to know what the effect of changing the value of $\\beta_1$ will have on the log loss function we can find this with the partial derivative:\n",
        "<center>$\n",
        "        \\dfrac{\\partial J}{\\partial \\beta_1}\n",
        "$</center>\n",
        "\n",
        "This may not seem very helpful by itself - after all, $\\beta_1$ isn't even in the expression of $J$. But if we use the chain rule, we can rewrite the expression as:\n",
        "<center>\n",
        "        $\\dfrac{\\partial J}{\\partial \\hat{p}} \\cdot\n",
        "        \\dfrac{\\partial \\hat{p}}{\\partial \\theta} \\cdot\n",
        "        \\dfrac{\\partial \\theta}{\\partial \\beta_1}$\n",
        "</center>\n",
        "\n",
        "We'll spare you the math (feel free to verify it youself, however!):\n",
        "\n",
        "<center>$\\dfrac{\\partial J}{\\partial \\hat{p}} =  \\dfrac{\\hat{p} - y}{\\hat{p}(1-\\hat{p})}, \\quad\n",
        "        \\dfrac{\\partial \\hat{p}}{\\partial \\theta} = \\hat{p} (1-\\hat{p}), \\quad\n",
        "        \\dfrac{\\partial \\theta}{\\partial \\beta_1} = X_1 $\n",
        "</center>\n",
        "\n",
        "and thus \n",
        "<center>$\n",
        "        \\dfrac{\\partial J}{\\partial \\beta_1} = (\\hat{p} - y) \\cdot X_1\n",
        "$</center>\n",
        "\n",
        "We can calculate the partial derivative for each parameter in $\\theta$ which, as you may have realized, is simply the $\\theta$ gradient of $J$: $\\nabla_{\\theta}(J)$\n",
        "\n",
        "With all of this information, we can now write $\\nabla_{\\theta} J$ in terms of the error, the feature vector, and the number of samples we're training on!\n",
        "\n",
        "<a name=\"grad_eq\"></a>\n",
        "\n",
        "<center>$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\theta^{(k)}}) = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n}{ \\left ( \\hat{p}^{(k)}_{i} - y_{i} \\right ) \\mathbf{X}_{i}}$</center>\n",
        "\n",
        "Note that here $k$ represents the iteration of the parameters we are currently on.\n",
        "\n",
        "We now have a gradient we can calculate and use in the batch gradient descent method! The updated parameters will thus be:\n",
        "\n",
        "<a name=\"grad_descent\"></a>\n",
        "\n",
        "\\begin{align} \n",
        "{\\mathbf{\\theta}^{(k+1)}} = {\\mathbf{\\theta}^{(k)}} - \\eta\\,\\nabla_{\\theta^{(k)}}J(\\theta^{(k)})\n",
        "\\end{align}\n",
        "\n",
        "Where $\\eta$ is the learning rate parameter. It's also worth pointing out that $\\;\\hat{p}^{(k)}_i = \\sigma\\left(\\theta^{(k)}, X_i\\right) $"
      ],
      "metadata": {
        "id": "aO4Bkm1gFV3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to easily calculate the input to the logistic regression, we'll multiply the $\\theta$ vector with the X data, and as we have a non-zero bias  $\\beta_0$ we'd like to have an X matrix whose first column is filled with ones.\n",
        "\n",
        "\\begin{align}\n",
        "    X_{\\small{with\\ bias}} = \\begin{pmatrix}\n",
        "        1 & X_{1,0} & X_{2,0}\\\\\n",
        "        1 & X_{1,1} & X_{2,1}\\\\\n",
        "        &...&\\\\\n",
        "        1 & X_{1,n} & X_{2,n} \n",
        "        \\end{pmatrix}\n",
        "\\end{align}\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "ML4uik7sbdMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q7) Prepare the `X_with_bias` matrix (remember to use the `bin_X` data and not just `X`). Write a function called `predict` that takes in the parameter vector $\\theta$ and the `X_with_bias` matrix and evaluates the logistic function for each of the samples.**\n",
        "\n",
        "*Hint 1: You recently learned how to initialize arrays in the `Numpy` notebook [at this link](https://nbviewer.org/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/S1_2_Numpy.ipynb). There are many ways to add a columns of 1 to `bin_X`, for instance using [`np.concatenate`](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) or [`np.append`](https://numpy.org/doc/stable/reference/generated/numpy.append.html).*\n",
        "\n",
        "*Hint 2:  To clarify, the function `predict` calculates $\\hat{p}$ from $\\beta$ and $\\boldsymbol{X}$.*\n",
        "\n",
        "*Hint 3: In practice, to calculate the logistic function for each sample, you may follow the equations [higher up in the notebook](#logit) and (1) calculate $t$ from $\\beta$ and $\\boldsymbol{X_{\\mathrm{with\\ bias}}}$ before (2) applying the logistic function $\\sigma$ to $t$.*"
      ],
      "metadata": {
        "id": "sqwV5qgrisB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint 4: Pseudocode Snippet\n",
        "\n",
        "define predict_function(x_with_bias, theta_vector):\n",
        "  argument_for_logistic_function = dot_product(x_with_bias, theta_vector)\n",
        "  return logistic_function(argument_for_logistic_function)"
      ],
      "metadata": {
        "id": "opqgiTM2L0jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the X_with_bias matrix\n",
        "def add_ones(x):\n",
        "    return np.hstack((np.ones((x.shape[0], 1)), x))\n",
        "X_with_bias = add_ones(bin_X)"
      ],
      "metadata": {
        "id": "3b2oOJ5WKn5m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your function predict here\n",
        "def predict(X_with_bias,theta_vector):\n",
        "  arg = np.dot(X_with_bias,theta_vector)\n",
        "  return logistic(arg)"
      ],
      "metadata": {
        "id": "tBLryApsbatR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q8) Now that you have a `predict` function, write a `gradient_calc` function that calculates the gradient for the logistic function.**\n",
        "\n",
        "*Hint 1: You'll have to feed `theta`, `X`, and `y` to the `gradient_calc` function.*\n",
        "\n",
        "*Hint 2: You can use [this equation](#grad_eq) to calculate the gradient of the cost function.*"
      ],
      "metadata": {
        "id": "p6cPbu4LvVES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint 3: Pseudocode Snippet\n",
        "\n",
        "define gradient_calculator_function(y, X_with_bias, theta_vector):\n",
        "  # predicted values using theta and inputs\n",
        "  prediction = predict(x_with_bias,theta_vector)\n",
        "  \n",
        "  error = prediction - y\n",
        "\n",
        "  X_transpose = transpose(X)\n",
        "\n",
        "  number_of_predictions = len(predict)\n",
        "\n",
        "  assert number_of_predictions == len(y)\n",
        "\n",
        "  return dot_product(X_transpose, error) / number_of_predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "XRQNz-2nPGVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "def gradient_calc(bin_y,X_with_bias,theta_vector):\n",
        "  pred = predict(X_with_bias,theta_vector)\n",
        "  error = pred - bin_y\n",
        "  X_transpose = np.transpose(X_with_bias)\n",
        "  n_pred = len(pred)\n",
        "  assert n_pred == len(bin_y)\n",
        "  return np.dot(X_transpose,error) / n_pred\n"
      ],
      "metadata": {
        "id": "BtnANN5WvVuy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now write a function that will train a logistic regression algorithm!\n",
        "\n",
        "Your `logistic_regression` function needs to:\n",
        "* Take in a set of training input/output data, validation input/output data, a number of iterations to train for, a set of initial parameters $\\theta$, and a learning rate $\\eta$\n",
        "* At each iteration:\n",
        " * Generate a set of predictions on the training data. Hint: You may use your function `predict` on inputs `X_train` from the training set.\n",
        " * Calculate and store the loss function for the training data at each iteration. Hint: You may use your function `log_loss` on inputs `X_train` and outputs `y_train` from the training set.\n",
        " * Calculate the gradient. Hint: You may use your function `grad_calc`.\n",
        " * Update the $\\theta$ parameters. Hint: You need to implement [this equation](#grad_descent).\n",
        " * Generate a set of predictions on the validation data using the updated parameters. Hint: You may use your function `predict` on inputs `X_valid` from the validation set. \n",
        " * Calculate and store the loss function for the validation data. Hint: You may use your function `log_loss` on inputs `X_valid` and outputs `y_valid` from the validation set. \n",
        " * Bonus: Calculate and store the accuracy of the model on the training and validation data as a metric!\n",
        "* Return the final set of parameters $\\theta$ & the stored training/validation loss function values (and the accuracy, if you did the bonus)\n",
        "\n"
      ],
      "metadata": {
        "id": "PU4A5HVKuAGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q9) Write the `logistic_regression` function**"
      ],
      "metadata": {
        "id": "182qtGB7i_vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint: Pseudocode Snippet\n",
        "\n",
        "define logistic_regression(\n",
        "                           X_test,\n",
        "                           y_test,\n",
        "                           X_validation,\n",
        "                           y_validation,\n",
        "                           theta_vector,\n",
        "                           number_of_iterations,\n",
        "                           learning_rate_eta\n",
        "                          ):\n",
        "  #initialize the list of losses\n",
        "  training_losses = list()\n",
        "  validation_losses = list()\n",
        "\n",
        "  for iteration in range(number_of_iterations):\n",
        "    train_set_predictions = predict(X_train, theta_vector)\n",
        "    train_loss = log_loss(train_set_predictions, y_train)\n",
        "    training_losses.append(train_loss)\n",
        "\n",
        "    gradient = gradient_calculator(y_train, X_train, theta_vector)\n",
        "    theta = theta - gradient * learning_rate_eta\n",
        "\n",
        "    validation_set_predictions = predict(X_validation, theta_vector)\n",
        "    validation_loss = log_loss(validation_set_predictions, y_validation)\n",
        "    validation_losses.append(validation_loss)\n",
        "\n",
        "    print(Completed (iteration)/(number_of_iterations)*100%)\n",
        "\n",
        "    return train_loss, validation_loss, theta"
      ],
      "metadata": {
        "id": "eNfODgtZYm2V",
        "outputId": "d70b8cfa-a8fa-4e78-ad5b-6739f9f8afd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-ebe3251aa1ae>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    define logistic_regression(\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "def logistic_regression(\n",
        "                           X_test,\n",
        "                           y_test,\n",
        "                           X_validation,\n",
        "                           y_validation,\n",
        "                           theta_vector,\n",
        "                           number_of_iterations,\n",
        "                           learning_rate_eta\n",
        "                          ):\n",
        "  #initialize the list of losses\n",
        "  training_losses = list()\n",
        "  validation_losses = list()\n",
        "\n",
        "  for iteration in range(number_of_iterations):\n",
        "    train_set_predictions = predict(X_train, theta_vector)\n",
        "    train_loss = log_loss(train_set_predictions, y_train)\n",
        "    training_losses.append(train_loss)\n",
        "\n",
        "    gradient = gradient_calc(y_train, X_train, theta_vector)\n",
        "    theta = theta_vector - gradient * learning_rate_eta\n",
        "\n",
        "    validation_set_predictions = predict(X_validation, theta_vector)\n",
        "    validation_loss = log_loss(validation_set_predictions, y_validation)\n",
        "    validation_losses.append(validation_loss)\n",
        "\n",
        "    print(f'Completed{(iteration/number_of_iterations)*100:.1f}')\n",
        "    return train_loss, validation_loss, theta"
      ],
      "metadata": {
        "id": "HDsR5TxPt-0Y"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Â¡Â¡Â¡Important Note!!!**\n",
        "\n",
        "The notebook assumes that you will return \n",
        "1. a Losses list, where Losses[0] is the training loss and Losses[1] is the validation loss\n",
        "2. a tuple with the 3 final coefficients ($\\beta_0$, $\\beta_1$, $\\beta_2$)\n",
        "\n",
        "The code for visualizing the bonus accuracy is not included - but it should be simple enough to do in a way similar to that which is done with the losses.\n",
        "\n",
        "---------------------"
      ],
      "metadata": {
        "id": "EWMDLk7wFB0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our logistic regression function, we're all set to train our algorithm! Or are we?\n",
        "\n",
        "There's an important data step that we've neglected up to this point - we need to split the data into the train, validation, and test datasets."
      ],
      "metadata": {
        "id": "2ep5FQYBmqG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = rnd_gen.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = bin_y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = bin_y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = bin_y[rnd_indices[-test_size:]]"
      ],
      "metadata": {
        "id": "CVrXzjYA2iil"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready! \n",
        "\n",
        "###**Q10) Train your logistic regression algorithm. Use 1400 iterations, $\\eta$=0.1**\n",
        "\n",
        "*Hint: It's time to use the `logistic_regression` function you defined in Q5.*"
      ],
      "metadata": {
        "id": "33IhRpME8LOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "number_of_iterations = 1400\n",
        "learning_rate_eta = 0.1\n",
        "train_losses,valid_losses, coeffs = logistic_regression(X_test,\n",
        "                                     y_test,\n",
        "                                     X_valid,\n",
        "                                     y_valid,\n",
        "                                     theta_vector,\n",
        "                                     number_of_iterations,\n",
        "                                     learning_rate_eta)"
      ],
      "metadata": {
        "id": "dWAr0ORYEYi2",
        "outputId": "035acec2-b1ce-4dfd-a38b-017589cddb77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the term inside the brackets is (60,)\n",
            "The size of the term inside the brackets is (20,)\n",
            "Completed0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how our model did while learning!"
      ],
      "metadata": {
        "id": "e7WHcpPiEcIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Produce the Loss Function Visualization Graphs\n",
        "fig, ax = plt.subplots(figsize=(18,8))\n",
        "ax.plot(train_losses, color='blue', label='Training', linewidth=3);\n",
        "ax.plot(valid_losses, color='orange', label='Validation', linewidth=3);\n",
        "ax.legend();\n",
        "ax.set_ylabel('Log Loss')\n",
        "ax.set_xlabel('Iterations')\n",
        "ax.set_title('Loss Function Graph')\n",
        "ax.autoscale(axis='x', tight=True)\n",
        "fig.tight_layout();\n",
        "\n",
        "# Let's get predictions from our model for the training, validation, and testing\n",
        "# datasets\n",
        "y_hat_train = (predict(X_train, coeffs)>=.5).astype(int)\n",
        "y_hat_valid = (predict(X_valid, coeffs)>=.5).astype(int)\n",
        "y_hat_test = (predict(X_test, coeffs)>=.5).astype(int)\n",
        "\n",
        "y_sets = [ [y_hat_train, y_train],\n",
        "           [y_hat_valid, y_valid],\n",
        "           [y_hat_test, y_test] ]\n",
        "\n",
        "def accuracy_score(y_hat, y):\n",
        "    assert(y_hat.size==y.size)\n",
        "    return (y_hat == y).sum()/y.size\n",
        "accuracies= list()\n",
        "[accuracies.append(accuracy_score(y_set[0],y_set[1])) for y_set in y_sets]\n",
        "\n",
        "printout= (f'Training Accuracy:{accuracies[0]:.1%} \\n'\n",
        "           f'Validation Accuracy:{accuracies[1]:.1%} \\n')\n",
        "\n",
        "# Add the testing accuracy only once you're sure that your model works!\n",
        "\n",
        "\n",
        "print(printout)"
      ],
      "metadata": {
        "id": "4wXFzZPjFjOn",
        "outputId": "415e14b9-f9d1-4d3b-8718-9a2be190044a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:46.7% \n",
            "Validation Accuracy:55.0% \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations on training a logistic regression algorithm from scratch! \n",
        "\n",
        "Your loss function graph should look something similar to this...\n",
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/EQBRz0U01L9BuBld6H8lnUoBVJMbCctgbFK5WVFp3d4SYw?download=1'>\n",
        "\n",
        "Once you're done with the upcoming environmental science applications notebook, feel free to come back to take a look at the challenges ðŸ˜€"
      ],
      "metadata": {
        "id": "4zfXs8M8Osie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges\n",
        "\n",
        "* **C1)** Add L2 Regularization to training function \n",
        "\n",
        "* **C2)** Add early stopping to the training algorithm! Stop training when the accuracy is >=90%\n",
        "\n",
        "* **C3)** Implement a softmax regression model (It's multiclass logistic regression ðŸ™‚)"
      ],
      "metadata": {
        "id": "VAa4bzT7PHRG"
      }
    }
  ]
}